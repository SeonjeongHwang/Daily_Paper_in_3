1~3 paragraphs of comments including questions, weakness, or new ideas

[2024.09.12]
### Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)
JMLR 2020, <https://arxiv.org/pdf/1910.10683>   
+ A unified framework that all text-based language problems into a text-to-text format.
+ pre-training causes the model to develop general-purpose abilities and knowledge.
+ Compare the effectiveness of different transfer learing objectives, unlabeled datasets, and other factors.
+ T5는 Transformer의 original architecture에서 layer norm과 position embedding scheme이 조금 수정됨

Question: English pretraining data만 활용했다는데, translation task는 어떻게?? 다른 언어에 대한 vocab의 embedding이 어떻게 있지?
