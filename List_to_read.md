### Interesting Keywords
_LLM continual learning, Vocabulary Expansion, Instruction Tuning_

[2024.07.03]   
~~Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models~~ (https://arxiv.org/pdf/2402.14714)   
~~Direct Preference Optimization: Your Language Model is Secretly a Reward Model~~ (https://arxiv.org/pdf/2305.18290)   

--------------------------------------------   
**[Paraphrase Generation]**   
Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Models (https://aclanthology.org/2024.naacl-long.250.pdf)   
MCPG: A Flexible Multi-Level Controllable Framework for Unsupervised Paraphrase Generation (https://aclanthology.org/2022.findings-emnlp.439.pdf)   
~~Quality Controlled Paraphrase Generation~~ (https://aclanthology.org/2022.acl-long.45.pdf)   
Dictionary-Guided Editing Networks for Paraphrase Generation (https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.33016546)    
~~Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level Knowledge Distillation~~ (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10473289)   
Paraphrase Generation with Deep Reinforcement Learning (https://aclanthology.org/D18-1421.pdf)   
User-Oriented Paraphrase Generation With Keywords Controlled Network (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8736871)   
Neural Syntactic Preordering for Controlled Paraphrase Generation (https://aclanthology.org/2020.acl-main.22.pdf)

**[LLM Knowledge Distillation]**   
LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions (https://aclanthology.org/2024.eacl-long.57.pdf)   

**[LLM Fine-tuning]**   
Fine-Tuning Language Models from Human Preferences (https://arxiv.org/pdf/1909.08593)
