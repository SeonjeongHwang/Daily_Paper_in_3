### Interesting Keywords
_LLM continual learning, Vocabulary Expansion, Instruction Tuning_

[2024.07.03]   
~~Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models (https://arxiv.org/pdf/2402.14714)~~   
Direct Preference Optimization: Your Language Model is Secretly a Reward Model (https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf)   

--------------------------------------------   
**[Paraphrase Generation]**   
Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Models (https://aclanthology.org/2024.naacl-long.250.pdf)   
MCPG: A Flexible Multi-Level Controllable Framework for Unsupervised Paraphrase Generation (https://aclanthology.org/2022.findings-emnlp.439.pdf)   
Quality Controlled Paraphrase Generation (https://aclanthology.org/2022.acl-long.45.pdf)   
Dictionary-Guided Editing Networks for Paraphrase Generation (https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.33016546)    
~~Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level Knowledge Distillation (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10473289)~~   
Paraphrase Generation with Deep Reinforcement Learning (https://aclanthology.org/D18-1421.pdf)   

**[LLM Knowledge Distillation]**   
LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions (https://aclanthology.org/2024.eacl-long.57.pdf)   
