### Interesting Keywords
_LLM continual learning, Vocabulary Expansion, Instruction Tuning_

[2024.07.04-05]   
~~Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models~~ (https://arxiv.org/pdf/2402.14714)   
~~Direct Preference Optimization: Your Language Model is Secretly a Reward Model~~ (https://arxiv.org/pdf/2305.18290)   

[2024.07.09-12]   
Gemma 2: Improving Open Language Models at a Practical Size (https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)   
LoRA: Low-rank adaptation of large language models (https://arxiv.org/pdf/2106.09685)   
~~Fine-grained Gender Control in Machine Translation with Large Language Models~~ (https://aclanthology.org/2024.naacl-long.303.pdf)   
MiniLLM: Knowledge Distillation of Large Language Models (https://arxiv.org/pdf/2306.08543)  

[2024.07.23-2024.07.26]   
Gemma 2: Improving Open Language Models at a Practical Size (https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)   
~~LoRA: Low-rank adaptation of large language models~~ (https://arxiv.org/pdf/2106.09685)  
MiniLLM: Knowledge Distillation of Large Language Models (https://arxiv.org/pdf/2306.08543)  
Evaluating Quantized Large Language Models (https://arxiv.org/pdf/2402.18158)   
~~Enhancing In-Context Learning via Implicit Demonstration Augmentation~~ (https://arxiv.org/pdf/2407.00100)

[2024.07.29-2024.07.33]   
MELA: Multilingual Evaluation of Linguistic Acceptability (https://arxiv.org/pdf/2311.09033)   
Probing Language Models for Pre-training Data Detection (https://arxiv.org/pdf/2406.01333)
Dissecting Human and LLM Preferences (https://arxiv.org/pdf/2402.11296)   
An Iterative Associative Memory Model for Empathetic Response Generation (https://arxiv.org/pdf/2402.17959)   
UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages (https://arxiv.org/pdf/2406.09717)  
Attribute First, then Generate: Locally-attributable Grounded Text Generation (https://arxiv.org/pdf/2403.17104)

**[Published Yet]**
Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models

--------------------------------------------   
**[Paraphrase Generation]**   
~~Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Models~~ (https://aclanthology.org/2024.naacl-long.250.pdf)   
~~MCPG: A Flexible Multi-Level Controllable Framework for Unsupervised Paraphrase Generation~~ (https://aclanthology.org/2022.findings-emnlp.439.pdf)   
~~Quality Controlled Paraphrase Generation~~ (https://aclanthology.org/2022.acl-long.45.pdf)   
~~Dictionary-Guided Editing Networks for Paraphrase Generation~~ (https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.33016546)    
~~Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level Knowledge Distillation~~ (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10473289)   
Paraphrase Generation with Deep Reinforcement Learning (https://aclanthology.org/D18-1421.pdf)   
User-Oriented Paraphrase Generation With Keywords Controlled Network (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8736871)   
Neural Syntactic Preordering for Controlled Paraphrase Generation (https://aclanthology.org/2020.acl-main.22.pdf)   
Unsupervised Paraphrasing with Pretrained Language Models (https://aclanthology.org/2021.emnlp-main.417/)   

**[LLM Knowledge Distillation]**   
LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions (https://aclanthology.org/2024.eacl-long.57.pdf)   

**[LLM Fine-tuning]**   
Fine-Tuning Language Models from Human Preferences (https://arxiv.org/pdf/1909.08593)

**[Constrained Generation]**   
NEUROLOGIC A Fesque Decoding: Constrained Text Generation with Lookahead Heuristics (https://aclanthology.org/2022.naacl-main.57.pdf)  
Generating Summaries with Controllable Readability Levels (https://arxiv.org/pdf/2310.10623)

**[NLI Data Generation]**   
Beyond the Obvious Multi-choice Options: Introducing a Toolkit for Distractor Generation Enhanced with NLI Filtering (https://link.springer.com/chapter/10.1007/978-3-031-64299-9_18)   
MENLI: Robust Evaluation Metrics from Natural Language Inference (https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00576/116715)   
~~Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization~~ (https://arxiv.org/pdf/2205.06009)   
~~NonFactS: NonFactual Summary Generation for Factuality Evaluation in Document Summarization~~ (https://aclanthology.org/2023.findings-acl.400.pdf)    
~~AMRFACT: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation~~ (https://aclanthology.org/2024.naacl-long.33.pdf)   
~~BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples~~ (https://arxiv.org/pdf/2210.14814)   
On the Intractability to Synthesize Factual Inconsistencies in Summarization (https://aclanthology.org/2024.findings-eacl.69.pdf)   
Evaluating the Factual Consistency of Abstractive Text Summarization (https://aclanthology.org/2020.emnlp-main.750.pdf)   
A Synthetic Data Approach for Domain Generalization of NLI Models (https://arxiv.org/pdf/2402.12368)   
ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions (https://arxiv.org/pdf/2406.04286)   

